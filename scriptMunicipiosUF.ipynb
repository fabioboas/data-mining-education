{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54ce739",
   "metadata": {},
   "source": [
    "# Importanto as libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae8a8958-c8f6-441b-b2cc-b626b042316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from anticaptchaofficial.recaptchav2proxyless import *\n",
    "from chave import google_api, search_id\n",
    "import datetime\n",
    "import requests\n",
    "import logging\n",
    "import cloudscraper\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Nível de log DEBUG para capturar tudo\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"debug_log.txt\"),  # Grava logs em um arquivo\n",
    "        logging.StreamHandler()  # Exibe logs no console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8643a00-9673-4c33-a27a-4130e93531d5",
   "metadata": {},
   "source": [
    "# Coleta o horário de inicio da aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5c5742f4-97ff-4d60-a5d4-be090916ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b4038-a736-4605-a0bf-57fee04dc08e",
   "metadata": {},
   "source": [
    "# Abrir, ler e transformar informações necessárias para buscar as URLs de acordo com arquivo do cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2b292a23-fb77-470f-810e-de94c059af37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ler a lista de sites e urls para a busca no google\n",
    "municipios = pd.read_excel('[LEVANTAMENTO MUNICÍPIOS].xlsx')\n",
    "municipio_nomes_uf = municipios.set_index('Nome Municipio')['Sigla UF'].to_dict()\n",
    "\n",
    "###Testes com grupos menores de dados\n",
    "\n",
    "# base_teste_municipios = list(municipio_nomes_uf.items())[0:4]\n",
    "# display(base_teste_municipios)\n",
    "# display(municipio_nomes_uf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8e2ef8-38ae-4dba-a3da-5cfe8176a1d9",
   "metadata": {},
   "source": [
    "# Coleta de todas as URLs das secretarias de educação a partir da lista do cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e7fdad52-921b-4cce-aca2-dc203085b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sites indesejados\n",
    "def is_not_desired_site(url):\n",
    "    undesired_domains = [\n",
    "        \"facebook.com\", \"twitter.com\", \"instagram.com\",\"glassdoor.com.br\",\"linkedin.com\", \"tiktok.com\", \"snapchat.com\",\n",
    "        \"econodata.com.br\",\"econodata.com.br\",\"serasaexperian.com.br\",\"diariomunicipal.com.br\",\"wikipedia.org\",\n",
    "        \"diariomunicipio.com.br\", \"validator.w3.org\",\"addtoany.com\",\"whatsapp.com\",\"pege.com.br\",\"sites.google.com\"\n",
    "        ]\n",
    "    return not any(domain in url for domain in undesired_domains)\n",
    "\n",
    "#Relacionados a educação\n",
    "def is_related_secretary_edu(url):\n",
    "    education_keywords = [\"gov.br\"]\n",
    "    return any(keyword in url for keyword in education_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75acc4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_urls_google(query, api_key, search_id):\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"key\": api_key,\n",
    "        \"cx\": search_id,\n",
    "        \"num\": 5,\n",
    "        \"lr\": \"lang_pt\",  # Filtra por páginas em português\n",
    "        \"cr\": \"countryBR\"  # Filtra por páginas do Brasil\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        results = response.json()\n",
    "        logger.debug(f\"Resposta completa da API para a query '{query}': {results}\")  # Log da resposta completa\n",
    "        if \"items\" not in results:\n",
    "            logger.warning(f\"Nenhum resultado encontrado para a query: {query}\")\n",
    "            return []\n",
    "        return [item[\"link\"] for item in results.get(\"items\", [])]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Erro ao buscar URLs no Google: {e}\")\n",
    "        return []\n",
    "    \n",
    "sites_secretarias = {}\n",
    "for municipio_nome, uf in municipio_nomes_uf.items():\n",
    "    query = f\"secretaria de educação de {municipio_nome} - {uf}\"\n",
    "    urls = buscar_urls_google(query, google_api, search_id)\n",
    "    \n",
    "    if urls:\n",
    "        for url in urls:\n",
    "            if is_not_desired_site(url) and is_related_secretary_edu(url):\n",
    "                sites_secretarias[municipio_nome] = url\n",
    "                logger.info(f\"URL válida encontrada para {municipio_nome} - {uf}: {url}\")\n",
    "                break\n",
    "        else:\n",
    "            logger.warning(f\"Nenhum link válido encontrado para {municipio_nome} - {uf}\")\n",
    "    else:\n",
    "        logger.warning(f\"Nenhum resultado encontrado no Google para {municipio_nome} - {uf}\")\n",
    "\n",
    "# Log para verificação das URLs\n",
    "log_file_dir = \"./\"\n",
    "log_file_name = \"log_urls_\"\n",
    "log_timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "log_file_ext = \".txt\"\n",
    "log_file_path = log_file_dir + log_file_name + log_timestamp + log_file_ext\n",
    "\n",
    "# Abre o arquivo de log no modo de escrita\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    log_file.write(\"Log de Verificação e Registro das URLs\\n\")\n",
    "    log_file.write(\"*\" * 5 + \"=\" * 90 + \"*\" * 5 + \"\\n\")\n",
    "\n",
    "    for municipio_nome, site_url in sites_secretarias.items():\n",
    "        log_entry = f\"{municipio_nome}-{uf}: {site_url}\\n\"\n",
    "        log_file.write(log_entry)\n",
    "\n",
    "logger.info(f\"Log gravado em {log_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad27f63-f61b-4823-9a41-5ef4f19757b6",
   "metadata": {},
   "source": [
    "# Acessa, busca, coleta e trata os dados das URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fdca75-ac1c-470a-9b57-0c8c161f8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Global\n",
    "search_tags = [\n",
    "    \"p\",\"br\",\"tr\",\"td\",\"strong\", \"span\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"div\", \"a\", \"li\", \"header\", \"footer\",\"body\",\n",
    "    \"tbody\",\"em\"\n",
    "    ]\n",
    "\n",
    "contact_patterns = [\n",
    "    \"organizac\", \"secretaria\", \"informacoes\", \"contact\", \"atendimento\",\"recado\", \"deixe seu recado\"\n",
    "    \"administrativo\", \"contato\", \"entidade\", \"conosco\", \"respostas\",\"institucional\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    headers = {\n",
    "        'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "                       '(KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "    contact_keywords = [\n",
    "        \"secretaria\", \"educacao\", \"sme\", \"seduc\", \"semec\", \"see\", \"mec\", \"rede\", \"escola\", \"municipal\", \"estadual\", \"smec\",\n",
    "        \"cme\", \"telefone\", \"resposta\", \"telefones\", \"telefone(s)\", \"semed\", \"sedu\", \"seceduc\", \"educasec\", \"secretario\", \n",
    "        \"sed\", \"seceduc\", \"smed\", \"comec\", \"contato\", \"faleconosco\", \"respostas\", \"prefeitura\", \"administracao\", \"celular\", \n",
    "        \"cel\", \"mobile\", \"phone\", \"educa\", \"educ\", \"fone\", \"Fone\", \"semeg\", \"seme\", \"Telefone\", \"cacs\", \"fale\", \"conosco\",\n",
    "        \"atendimento\", \"E-mail da unidade\", \"conselho\", \"SUPERINTENDÊNCIA\", \"superintendencia\", \"pop\", \"tel\", \"gmail\", \n",
    "        \"hotmail\", \"yahoo\", \"bol\", \"Fale Conosco\", \"fundeb\", \"fale conosco\", \"fax\", \"gabinete\", \"gab\", \"mc\", \"mc3\", \n",
    "        \"outlook\", \"secedu\", \"seceducacao\", \"mail\", \"comunicacao\", \"respostasgabinetesme\",\"adm\"\n",
    "    ]\n",
    "\n",
    "    contacts = gather_contacts_from_sites(sites_secretarias, headers, contact_keywords)\n",
    "    \n",
    "    data = []\n",
    "    for municipio, site in sites_secretarias.items():\n",
    "        contact_info = contacts.get(municipio, {})\n",
    "        email_str = ', '.join(contact_info.get('emails', []))\n",
    "        phone_str = ', '.join(contact_info.get('phones', []))\n",
    "        data.append([municipio, site, email_str or \"E-mail não encontrado - Verifique manualmente\",\n",
    "                     phone_str or \"Telefone não encontrado - Verifique manualmente\"])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"Município\", \"Site da Secretaria\", \"E-mails de Contato\", \"Telefone de Contato\"])\n",
    "\n",
    "    needs_retry = df[(df[\"E-mails de Contato\"].str.contains(\"E-mail não encontrado\")) | \n",
    "                     (df[\"Telefone de Contato\"].str.contains(\"Telefone não encontrado\"))]\n",
    "\n",
    "    processed_urls = set()  # Inicializa o conjunto fora do loop\n",
    "\n",
    "    for index, row in needs_retry.iterrows():\n",
    "        url = row[\"Site da Secretaria\"]\n",
    "        if url in processed_urls:\n",
    "            continue  # Evita reprocessamento\n",
    "\n",
    "        processed_urls.add(url)  # Adiciona a URL ao conjunto de URLs processadas\n",
    "\n",
    "        content = load_page_content(url, headers)\n",
    "        if content:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            contact_page_urls = find_contact_pages(soup, url, contact_patterns)\n",
    "\n",
    "            for contact_page_url in contact_page_urls:\n",
    "                if contact_page_url in processed_urls or contact_page_url == url:\n",
    "                    continue  # Evita reprocessamento\n",
    "\n",
    "                processed_urls.add(contact_page_url)\n",
    "                print(f\"Processing URL: {contact_page_url}\")\n",
    "                \n",
    "                page_content = load_page_content(contact_page_url, headers)\n",
    "                if page_content:\n",
    "                    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "                    new_emails = get_emails_from_page(soup, contact_keywords)\n",
    "                    new_phones = get_phones_from_page(soup, contact_keywords)\n",
    "                    \n",
    "                    if \"E-mail não encontrado\" in row[\"E-mails de Contato\"]:\n",
    "                        df.at[index, 'E-mails de Contato'] = ', '.join(new_emails) or \"E-mail não encontrado - Verifique manualmente\"\n",
    "                    if \"Telefone não encontrado\" in row[\"Telefone de Contato\"]:\n",
    "                        df.at[index, 'Telefone de Contato'] = ', '.join(new_phones) or \"Telefone não encontrado - Verifique manualmente\"\n",
    "    \n",
    "    create_excel(df)\n",
    "\n",
    "def find_contact_pages(soup, url, contact_patterns):\n",
    "    found_links = set()\n",
    "    ignored_strings = [ \n",
    "        ###Com as palavras chaves necessarias para o montante do projeto,\n",
    "        ###os sites abaixo oneram muito tempo para achar um contato e a taxa de retorno é baixissíma, \n",
    "        ###por isso removo-os\n",
    "        \"educacao.sp.gov.br\",\n",
    "        \"educacao.ba.gov.br\",\n",
    "        \"educacao.mg.gov.br\"\n",
    "    ]\n",
    "    \n",
    "    for tag in soup.find_all('a', href=True):\n",
    "        href = tag['href'].strip()\n",
    "        if href.lower().startswith(\"javascript:\") or href.startswith(\"#\"):\n",
    "            continue\n",
    "        #Processa o link completo\n",
    "        full_url = urllib.parse.urljoin(url, href)\n",
    "        #Verificar se o link contém qualquer uma das strings ignoradas\n",
    "        if any(ignored_string in full_url for ignored_string in ignored_strings):\n",
    "            continue\n",
    "        if is_not_desired_site(full_url) and is_related_secretary_edu(full_url):\n",
    "            href_lower = href.lower()\n",
    "            if any(keyword in href_lower for keyword in contact_patterns):\n",
    "                found_links.add(full_url)\n",
    "    \n",
    "    print(\"Found links:\", found_links)\n",
    "    return list(found_links)\n",
    "\n",
    "def create_excel(df):\n",
    "    excel_filename = \"Contatos_secretarias_de_educacao_\"\n",
    "    excel_timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    df.to_excel(excel_filename + excel_timestamp + \".xlsx\", index=False)\n",
    "    print(f\"Arquivo salvo como: {excel_filename + excel_timestamp + '.xlsx'}\")\n",
    "\n",
    "###Para deixar o site \"limpo\" para a coleta de dados\n",
    "def is_cloudflare_protected(response_text):\n",
    "    return \"Checking your browser\" in response_text or \"Cloudflare\" in response_text\n",
    "\n",
    "def bypass_cloudflare(driver, url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    return driver.page_source\n",
    "\n",
    "def is_security_warning_page(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 2).until(\n",
    "            EC.presence_of_element_located((By.ID, \"proceed-link\"))\n",
    "        )\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def disable_undesired_blocks(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.ID, \"proceed-link\"))).click()\n",
    "    except Exception as e:\n",
    "        print(\"Restrição não encontrada ou outro erro ocorreu:\", e)\n",
    "\n",
    "def get_page_content(url):\n",
    "    driver = None\n",
    "    try:\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--disable-plugins-discovery\")\n",
    "        chrome_options.add_argument(\"--disable-extensions\")\n",
    "        chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "        chrome_options.add_argument(\"--disable-notifications\")\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        if is_security_warning_page(driver):\n",
    "            disable_undesired_blocks(driver)\n",
    "\n",
    "        content_after_blocks_removed = driver.page_source\n",
    "\n",
    "        if not content_after_blocks_removed:\n",
    "            print(f\"Não foi possível obter o conteúdo da página {url} após desabilitar blocos.\")\n",
    "            return None\n",
    "        \n",
    "        if is_cloudflare_protected(content_after_blocks_removed):\n",
    "            print(\"Página protegida por Cloudflare detectada. Usando Selenium para bypass.\")\n",
    "            return bypass_cloudflare(driver, url)        \n",
    "\n",
    "        return content_after_blocks_removed\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "def wait_for_page_load_with_cloudflare(url, headers, max_attempts=10, sleep_interval=3):\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            response = scraper.get(url, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if is_cloudflare_protected(response.text):\n",
    "                logger.info(\"Proteção Cloudflare detectada. Tentando pela Selenium.\")\n",
    "                driver = webdriver.Chrome()\n",
    "                driver.get(url)\n",
    "\n",
    "                disable_undesired_blocks(driver)\n",
    "\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "                response_text = driver.page_source\n",
    "                driver.quit()\n",
    "\n",
    "                return response_text\n",
    "\n",
    "            logger.info(f\"Página carregada com sucesso na tentativa nº {attempts + 1}: {url}\")\n",
    "            return response.text\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            logger.error(f\"Erro HTTP ao acessar {url}: {e}\")\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            logger.error(f\"Erro de conexão ao acessar {url}: {e}\")\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            logger.error(f\"Timeout ao acessar {url}: {e}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Erro geral na requisição ao acessar {url}: {e}\")\n",
    "\n",
    "        attempts += 1\n",
    "        logger.info(f\"Tentativa nº {attempts}: Página não carregou. Aguardando...\")\n",
    "        time.sleep(sleep_interval)\n",
    "\n",
    "    logger.error(f\"Página não carregou após {max_attempts} tentativas. URL: {url}\")\n",
    "    return None\n",
    "\n",
    "def load_page_content(url, headers, max_attempts=10, sleep_interval=3):\n",
    "    attempts = 0\n",
    "\n",
    "    #Verifica primeiramente se a página é protegida ou dinamicamente carregada\n",
    "    try:\n",
    "        initial_response = requests.get(url, headers=headers, timeout=10)\n",
    "        if is_cloudflare_protected(initial_response.text) or is_dynamically_loaded(initial_response.text):\n",
    "            logger.info(\"Página protegida ou carregada dinamicamente. Tentando usar Selenium.\")\n",
    "            return fetch_with_selenium(url)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Erro na verificação inicial da página {url}: {e}\")\n",
    "\n",
    "    #Se não for protegida ou dinamicamente carregada, usa cloudscraper    \n",
    "    scraper = cloudscraper.create_scraper()\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            response = scraper.get(url, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            logger.info(f\"Página carregada com sucesso na tentativa nº {attempts + 1}: {url}\")\n",
    "            return response.text\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Erro ao acessar {url}: {e}\")\n",
    "\n",
    "        attempts += 1\n",
    "        logger.info(f\"Tentativa nº {attempts}: Página não carregou. Aguardando...\")\n",
    "        time.sleep(sleep_interval)\n",
    "\n",
    "    logger.error(f\"Página não carregou após {max_attempts} tentativas. URL: {url}\")\n",
    "    return None\n",
    "\n",
    "def fetch_with_selenium(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        disable_undesired_blocks(driver)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        return driver.page_source\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def is_dynamically_loaded(response_text):\n",
    "    #Verifica se a página contém muitos scripts, sugerindo carregamento dinâmico\n",
    "    script_count = response_text.count('<script')\n",
    "    #Procura por padrões comuns de carregamento dinâmico\n",
    "    dyn_keywords = [\n",
    "        'ajax', 'setTimeout(', 'setInterval(', 'XMLHttpRequest',\n",
    "        'fetch(', 'Vue', 'React', 'Angular', 'app-root', 'ng-app', 'ng-controller'\n",
    "    ]\n",
    "    #Conta ocorrências de palavras-chave relacionadas a JS dinâmico\n",
    "    keyword_presence = any(keyword in response_text for keyword in dyn_keywords)\n",
    "    #Determina que a página é dinamicamente carregada se houver muitos scripts ou palavras-chave indicando isso\n",
    "    return script_count > 10 or keyword_presence\n",
    "\n",
    "###Angariando os contatos\n",
    "def gather_contacts_from_sites(sites_secretarias, headers, contact_keywords):\n",
    "    contacts = {}\n",
    "\n",
    "    for municipio, url in sites_secretarias.items():\n",
    "        if url:\n",
    "            try:\n",
    "                page_content = wait_for_page_load_with_cloudflare(url, headers)\n",
    "                found_emails, found_phones = set(), set()\n",
    "\n",
    "                if page_content:\n",
    "                    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "                    found_emails = get_emails_from_page(soup, contact_keywords)\n",
    "                    found_phones = get_phones_from_page(soup, contact_keywords)\n",
    "\n",
    "                contacts[municipio] = {'emails': list(found_emails), 'phones': list(found_phones)}\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Erro de rede em {url}: {e}\")\n",
    "                contacts[municipio] = {'emails': [], 'phones': []}\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {url}: {str(e)}\")\n",
    "                contacts[municipio] = {'emails': [], 'phones': []}\n",
    "        else:\n",
    "            contacts[municipio] = {'emails': [], 'phones': []}\n",
    "    return contacts\n",
    "\n",
    "###Para coletar os emails\n",
    "def get_emails_from_page(soup, contact_keywords):\n",
    "    emails = set()\n",
    "    email_pattern = r\"[\\w.-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}\"\n",
    "\n",
    "    for tag in search_tags:\n",
    "        elements = soup.find_all(tag)\n",
    "        for element in elements:\n",
    "            element_text = element.get_text().lower()\n",
    "            if any(keyword in element_text for keyword in contact_keywords):\n",
    "                possible_emails = re.findall(email_pattern, element_text)\n",
    "                for email in possible_emails:\n",
    "                    sanitized_email = sanitize_email(email, contact_keywords)\n",
    "                    emails.update(sanitized_email)\n",
    "                if len(emails) >= len(contact_keywords):\n",
    "                    break\n",
    "        if len(emails) >= len(contact_keywords):\n",
    "            break\n",
    "\n",
    "    if len(emails) < len(contact_keywords):\n",
    "        possible_emails = re.findall(email_pattern, soup.get_text().lower())\n",
    "        for email in possible_emails:\n",
    "            sanitized_email = sanitize_email(email, contact_keywords)\n",
    "            emails.update(sanitized_email)\n",
    "\n",
    "    mailto_links = soup.select('a[href^=mailto]')\n",
    "    for link in mailto_links:\n",
    "        email = link['href'].split(':')[1]\n",
    "        sanitized_email = sanitize_email(email, contact_keywords)\n",
    "        emails.update(sanitized_email)\n",
    "\n",
    "    cfemail_elements = soup.select('.__cf_email__')\n",
    "    for element in cfemail_elements:\n",
    "        email = decode_email(element['data-cfemail'])\n",
    "        sanitized_email = sanitize_email(email, contact_keywords)\n",
    "        emails.update(sanitized_email)\n",
    "\n",
    "    return emails\n",
    "    \n",
    "def decode_email(encoded_string):\n",
    "    r = int(encoded_string[:2], 16)\n",
    "    email = ''.join(chr(int(encoded_string[i:i+2], 16) ^ r) for i in range(2, len(encoded_string), 2))\n",
    "    return email\n",
    "\n",
    "def sanitize_email(text, contact_keywords):\n",
    "    # Adiciona um espaço entre números e letras caso estejam grudados\n",
    "    text = re.sub(r'(\\d)([a-zA-Z])', r'\\1 \\2', text)\n",
    "    #Atualiza o padrão para números de telefone comuns no Brasil\n",
    "    phone_pattern = r'\\b(\\d{2,4}[-.()\\s]*){2,3}\\d{4}\\b'\n",
    "    #Remove números de telefone\n",
    "    sanitized_text = re.sub(phone_pattern, '', text)\n",
    "    #Corrige emails concatenados com \"brho\" ao final\n",
    "    sanitized_text = re.sub(r'\\b(br)ho\\b', r'\\1', sanitized_text)\n",
    "    #Captura todos os e-mails no texto\n",
    "    email_pattern = r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}\\b'\n",
    "    all_emails = re.findall(email_pattern, sanitized_text)\n",
    "    #Filtra e-mails válidos que terminam com \".br\" ou contêm um ano específico\n",
    "    valid_emails = [email for email in all_emails if email.endswith('.br') or re.search(r'\\d{4}', email)]\n",
    "    #Cria padrão regex que permite qualquer correspondência de palavras-chave\n",
    "    contact_pattern = '|'.join(re.escape(word) for word in contact_keywords)\n",
    "    #Verifica se os emails contêm qualquer uma das palavras-chave\n",
    "    filtered_emails = [email for email in valid_emails if re.search(contact_pattern, email)] \n",
    "    #Retorna todos os e-mails em minúsculas, evitando duplicações\n",
    "    return set(email.lower().strip() for email in filtered_emails)\n",
    "\n",
    "###Coletar os telefone e celulares\n",
    "def get_phones_from_page(soup, contact_keywords):\n",
    "    phones = set()\n",
    "    phone_pattern = r'(?:\\+?55\\s?)?\\(?\\d{2}\\)?[\\s.-]?9?\\d{4}[-\\s./]?\\d{4}'\n",
    "    undesired_patterns = [\n",
    "        r'\\b\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}\\b',  #CPF\n",
    "        r'\\b\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2}\\b',  #CNPJ\n",
    "        r'(?<![\\d/.-])\\b\\d{4}\\b(?!/)',  #Ano isolado\n",
    "        r'\\blei?\\b|\\blegislação\\b',  #Exemplo genérico para leis\n",
    "    ]\n",
    "\n",
    "    def is_undesired_number(number):\n",
    "        return any(re.search(pattern, number) for pattern in undesired_patterns)\n",
    "\n",
    "    found_using_keywords = False\n",
    "\n",
    "    for tag in search_tags:\n",
    "        elements = soup.find_all(tag)\n",
    "        for element in elements:\n",
    "            element_text = element.get_text().lower()\n",
    "            if any(keyword in element_text for keyword in contact_keywords):\n",
    "                found_phones = re.findall(phone_pattern, element_text)\n",
    "                for phone in found_phones:\n",
    "                    sanitized_phone = sanitize_phone(phone)\n",
    "                    if sanitized_phone and not is_undesired_number(sanitized_phone):\n",
    "                        phones.add(sanitized_phone)\n",
    "                if phones:\n",
    "                    found_using_keywords = True\n",
    "                    break\n",
    "        if phones:\n",
    "            break\n",
    "\n",
    "    if not phones:\n",
    "        full_text = soup.get_text().lower()\n",
    "        found_phones = re.findall(phone_pattern, full_text)\n",
    "        for phone in found_phones:\n",
    "            sanitized_phone = sanitize_phone(phone)\n",
    "            if sanitized_phone and not is_undesired_number(sanitized_phone):\n",
    "                phones.add(sanitized_phone)\n",
    "\n",
    "    return phones\n",
    "\n",
    "def sanitize_phone(phone):\n",
    "    # Remove tudo que não é número\n",
    "    clean_phone = re.sub(r'\\D', '', phone)\n",
    "    # Verifica se o número tem pelo menos 10 dígitos\n",
    "    if len(clean_phone) >= 10:\n",
    "        return clean_phone\n",
    "    return None\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2f4b1",
   "metadata": {},
   "source": [
    "# Limpando a pasta de logs e de arquivos - Últimos 3 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "069c93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dias que deseja deixar os logs e arquivos criados pelo main()\n",
    "dias_para_manter = 3\n",
    "\n",
    "#Calcula a data limite\n",
    "data_limite = datetime.datetime.now() - datetime.timedelta(days=dias_para_manter)\n",
    "\n",
    "def limpar_arquivos_antigos(pasta, prefixos, extensao):\n",
    "    for arquivo in os.listdir(pasta):\n",
    "        #Verifica se o arquivo tem a extensão e prefixo desejados\n",
    "        if arquivo.endswith(extensao) and any(arquivo.startswith(prefixo) for prefixo in prefixos):\n",
    "            #Obtém o caminho completo do arquivo\n",
    "            caminho_arquivo = os.path.join(pasta, arquivo)\n",
    "            #Obtém a data de modificação do arquivo\n",
    "            data_modificacao = datetime.datetime.fromtimestamp(os.path.getmtime(caminho_arquivo))\n",
    "            #Verifica se o arquivo é mais antigo que a data limite\n",
    "            if data_modificacao < data_limite:\n",
    "                os.remove(caminho_arquivo)\n",
    "                print(f\"Arquivo removido: {arquivo}\")\n",
    "\n",
    "#Caminho para a pasta onde os arquivos são salvos\n",
    "pasta_raiz = \"./\"\n",
    "\n",
    "#Chama a função para limpar arquivos antigos\n",
    "limpar_arquivos_antigos(pasta_raiz, [\"Contatos_secretarias_de_educacao_\", \"log_urls_\"], \".xlsx\")\n",
    "limpar_arquivos_antigos(pasta_raiz, [\"log_urls_\"], \".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fefc28d-2899-4dcd-b7b5-d325b4d0b7ca",
   "metadata": {},
   "source": [
    "# Contabilizando o tempo que a aplicação demorou para terminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e18bc487-74b3-4008-a104-89f9af4c208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:13:53\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "#Calcula o total de segundos\n",
    "total_seconds = end_time - start_time\n",
    "#Converte esse tempo em um objeto timedelta\n",
    "delta = datetime.timedelta(seconds=total_seconds)\n",
    "#Formata e converte o timedelta como HH:MM:SS - string\n",
    "total_time = str(delta).split('.')[0]\n",
    "\n",
    "print(total_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
